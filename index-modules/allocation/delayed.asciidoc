[[delayed-allocation]]
=== 节点失效后延迟分配

当一个节点不管是有意还是无意的原因脱离集群后，主节点有如下反应：

* 提升一个索引分片副本为主分片来替代该节点上的任何主分片。
* 重新分配索引分片副本以替代那些消失的索引分片副本。
* 在剩下的节点中重新平衡分片。

这些动作是希望通过让每个分片尽快得到备份以保护集群不丢失数据。

即使我们可以调节 <<recovery,节点级别>> 和 <<shards-allocation,集群级别>> 的并发恢复策略，
这种 “分片重排” 任然可能给集群带来额外的压力，这在离群节点会马上重新加入的情况下更是不必要的。
考虑如下场景：

* 节点 5 因为网络原因离开集群。
* 主节点对节点 5 上的每个主分片安排了一个新的主分片。
* 主节点分配新的分片副本给集群中其他节点。
* 每个分片副本所在节点通过网络总主分片上复制了一整份该分片的数据到本机。
* 更多的分片被移动到不同的节点以平衡集群。
* 节点 5 过了几分钟重新加入集群中。
* 主节点给节点 5分配分片以重新平衡节点。

如果主节点等待几分钟的话，那些丢失的分片就可以被重新分配给节点 5， 这将消耗最小的网络带宽。 这个
过程对那些正在做自动 <<indices-synced-flush,sync-flushed>>操作的 idle 分片
（不接收索引请求的分片）会更快。

未被分配的索引分片副本的重分配可以通过这个参数 `index.unassigned.node_left.delayed_timeout`
来动态设置，默认为 `1m`。

这个配置可以对一个正在运行的索引 （或所有索引）进行设置。

[source,js]
------------------------------
PUT /_all/_settings
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "5m"
  }
}
------------------------------
// AUTOSENSE

通过开启延迟分配，以上场景看上去会是这样的：

* 节点 5 因为网络原因离开集群。
* 主节点对节点 5上的主分片安排一个新的主分片。
* 主节点记录一条消息表明有未分配的分片被延迟分配了，和延迟的时间。
* 集群状态保持在黄色，因为还有未分配的分片副本。
* 节点 5 几分钟后重新加入集群，没有超过 `timeout` 时间。
* 丢失的分片重新被分配给节点 5 （分片几乎立即恢复）。

NOTE: 这个设置不会对主分片的重新选择产生影响，也不会对错误发生之前为分配的分片副本的分配产生影响。

==== 取消分片重分配

如果延迟重分配超时了，主节点会将丢失的分片分配给其他节点，这会导致分片恢复。 如果离开的节点重新
加入集群， 该节点上的分片和主分片上具有同样的 sync-id, 分片重分配会被取消，节点上已被同步的分
片会被用来做恢复。

由于这个原因， 默认的 `timeout` 只被设置为1分钟：即使分片重分配开始了，取消同步过的分片的恢复
也是简单的。

==== 监控未分配分片的延迟重分配

那些被延迟重分配的分片数量可以通过这个 <<cluster-health,cluster health API>>查看：
[source,js]
------------------------------
GET _cluster/health <1>
------------------------------
<1> 这个请求会返回一个 `delayed_unassigned_shards` 的值。

==== 永久移除一个节点

如果一个节点不会再加入集群，且你希望Elasticsearch立即重新分配丢失的分片，可以将 timeout设为0：

[source,js]
------------------------------
PUT /_all/_settings
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "0"
  }
}
------------------------------
// AUTOSENSE

一旦丢失分片开始了恢复，你可以将timeout重置为原来的值
